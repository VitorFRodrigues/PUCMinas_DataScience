# -*- coding: utf-8 -*-
"""Exploracao_de_dados.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vQmCsiDim_OIYUKPadMXiQeQvTjvdxJg

# Grupo 4

Instituição: Pontifícia Universidade Católica de Minas Gerais

Alunos:
1. Alessandro Augusto Bezerra
1. Isabela D'Loan
1. Robson Gomes de Lima
1. Vitor Fernando de Souza Rodrigues

Oferta: 07 - Turma: 1

Disciplina: Python para Ciência de Dados

Docente: Leandro Lessa

# 1. Introdução

No decorrer do tempo, as organizações vêm enfrentando maiores desafios sobre lidar com os seus respectivos volumes massivos de dados diariamente, exigindo tecnologias eficientes para análises. A ciência e a análise de dados são essenciais no cotidiano moderno. Elas permitem transformar vastas quantidades de dados em insights valiosos, ajudando a resolver problemas e a otimizar processos em diversos setores, como saúde, educação e negócios. Com a análise de dados, é possível prever tendências, personalizar experiências e auxiliar na tomada de decisão. No campo da ciência e análise de dados, várias ferramentas e bibliotecas têm sido desenvolvidas para aprimorar essas práticas. Exemplos disso são as bibliotecas Pandas e NumPy, da linguagem Python, que são cruciais para a manipulação e organização de dados. Matplotlib e Seaborn permitem criar visualizações que tornam os dados mais compreensíveis. Além disso, análises preditivas com machine learning e inteligência artificial possibilitam previsões precisas e automação de decisões. Portanto, essas tecnologias colaboram a transformar grandes volumes de dados em insights valiosos, capacitando as organizações a melhorar seus processos e decisões estratégicas visando maior precisão e confiança. Com isso, elas não apenas resolvem problemas imediatos, mas também se preparam para um futuro mais eficiente e inovador.

# 2. Criação de virtualenv e instalação das bibliotecas necessárias

Antes de iniciar a análise de dados, vamos criar nosso ambiente virtual através do `virtualenv`.

O `virtualenv` é uma ferramenta fundamental para a gestão de ambientes Python, especialmente em projetos que dependem de diferentes versões de bibliotecas.

Ele permite a criação de ambientes isolados, onde cada projeto pode ter suas próprias dependências, sem interferir nas configurações globais do sistema. Isso é crucial para evitar conflitos entre pacotes e garantir que os projetos funcionem corretamente com as versões específicas das bibliotecas necessárias. Além disso, o uso de `virtualenv` facilita a reprodução de ambientes de desenvolvimento e produção, contribuindo para um fluxo de trabalho mais eficiente e controlado.

## 2.1 Passos para criar máquina virtual do zero

1. Instalar Python na máquina (Marcar Add to Path)

2. criar pasta VIRTUALENVS e abrir ela no cmd

3. Instalar Virtual Env na máquina
```
pip install virtualenv
```

4. Dentro de uma pasta, criar a virtual env que deseja trabalhar:
```
virtualenv -p python3 pydstrabfinal
```

5. Para acessar a virtual env criada dentro da pasta em que estão os envs principal digite
```
pydstrabfinal\scripts\activate
```

6. Para ver as bibliotecas instaladas digite o comando:
```
pip list
```

7. Para instalar uma biblioteca digite: `pip install <biblioteca>`. Abaixo estão as bibliotecas instaladas para este trabalho:

  ```
  pip install ipykernel
  pip install pandas
  pip install matplotlib
  pip install searborn
  ```

8. Identificando e ativando a virtual env no sistema

  ```
  python -m ipykernel install –-user –-name=pydstrabfinal
  ```
9. Ao final do projeto, todas as bibliotecas podem ser compiladas em um único documento através da função:

  ```
  pip freeze > requirements.txt
  ```

## 2.2 Importação de bibliotecas

Abaixo estão todas as bibliotecas necessárias para este escopo:
"""

import pandas as pd
from pandas.api.types import is_string_dtype
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import requests
import warnings
warnings.filterwarnings('ignore')

"""# 3. Coleta de dados dos arquivos

O conjunto de dados foi baixado a partir do [link](https://leandrolessa.com.br/datasets/). Para facilitar a necessidade de download, os *datasets* foram upados para um site no github onde os dados crus ("raw") podem ser encontrados.

Estes arquivos (**características físicas** e **dados pessoais**) representam dados fictícios que contém informações detalhadas sobre os clientes de uma empresa, abrangendo uma ampla gama de aspectos, desde características demográficas até dados físicos e de saúde.
"""

URL = 'https://raw.githubusercontent.com/VitorFRodrigues/PUCMinas_DataScience/main/01-Python_para_ciencia_de_dados/Material%20-%20Alunos/Scripts/zTrabalhoFinal/datasets/dados_caracteristicas_fisicas.csv'
df_carac_fisica = pd.read_csv(URL)
df_carac_fisica.head()

print(f'Número total de linhas e colunas do dataset Características físicas: {df_carac_fisica.shape}')

URL = 'https://raw.githubusercontent.com/VitorFRodrigues/PUCMinas_DataScience/main/01-Python_para_ciencia_de_dados/Material%20-%20Alunos/Scripts/zTrabalhoFinal/datasets/dados_pessoais.csv'
df_pessoais = pd.read_csv(URL, sep=';', encoding='Latin')
df_pessoais.head()

print(f'Número total de linhas e colunas do dataset Dados Pessoais: {df_pessoais.shape}')

"""Além destes, vamos utilizar o conjunto de dados que compila em um único dataframe os estados brasileiros:"""

URL = 'https://leandrolessa.com.br/lista-estados-brasileiros/'

headers = {"User-Agent": "Brave"}
response = requests.get(URL, headers=headers)

if response.status_code == 200:
    # Use o método read_html do Pandas para extrair todas as tabelas da página
    html = response.text
    df_estados = pd.read_html(html, header=0, thousands='.')
    df_estados = df_estados[0] # Coleta a primeira tabela

else:
    # Se a solicitação não for bem-sucedida, imprima o código de status HTTP
    print("Erro ao solicitar a página:", response.status_code)

df_estados.head()

print(f'Número total de linhas e colunas do dataset Estados Brasileiros: {df_estados.shape}')

"""# 4. Tratamento dos dados coletados

Inicialmente, vamos realizar uma análise rápida nos dataframes, buscando entender os tipos de dados compostos em cada coluna e a existência de valores nulos.

## 4.1 Investigando os dados

Vamos realizar uma análise rápida a respeito das colunas dos dataframes passados com o intuito de identificar os pontos de trabalho e limpeza a ser realizado mais a frente.

- Características Físicas
"""

df_carac_fisica.info()

"""Analisando o *dataframe* de características físicas, vemos que não há valores nulos e 5 colunas do tipo objeto. Vamos identificar se estas colunas são do tipo categóricas:"""

colunas_tipo_obj = df_carac_fisica.select_dtypes(include='object').columns.to_list()

for col in colunas_tipo_obj:
    print(f'Coluna: {col} / Valores Categóricos: {df_carac_fisica[col].unique()}')

"""Em data science, uma coluna de dados categóricos contém valores que representam categorias ou grupos distintos e não possuem uma ordem intrínseca (dados nominais) ou possuem uma ordem natural (dados ordinais). Esses dados são qualitativos e descrevem atributos ou características que podem ser divididos em diferentes categorias.

Por exemplo, numa coluna de dados categóricos sobre a cor do cabelo de uma pessoa, você pode ter valores como:

- Castanho
- Preto
- Loiro
- Ruivo

Esses valores representam diferentes categorias de cabelo. Dependendo do contexto, essas categorias podem ser tratadas como dados nominais (se considerarmos apenas as categorias sem nenhuma ordem) ou ordinais (se considerarmos que há uma progressão no cabelo).

Os dados categóricos são frequentemente convertidos em uma forma numérica para uso em modelos de machine learning. Isso pode ser feito usando técnicas como codificação *one-hot* ou *codificação ordinal*, dependendo do tipo de dado categórico e do modelo que está sendo utilizado.

Agora vamos avaliar os dados numéricos:
"""

df_carac_fisica.describe()

"""Os dados numéricos, ao contrário dos categóricos, representam valores quantificáveis e são utilizados para medir ou contar algo. Eles são divididos em dois tipos principais:

1. **Dados Discretos**:
   - Representam contagens inteiras e não podem ser fracionados.
   - Exemplos incluem o número de alunos em uma sala de aula, o número de carros em um estacionamento, etc.

2. **Dados Contínuos**:
   - Podem assumir qualquer valor dentro de um intervalo contínuo e podem ser fracionados.
   - Exemplos incluem altura, peso, temperatura, tempo, etc.

Essas características tornam os dados numéricos extremamente versáteis e poderosos para análise e modelagem preditiva. Neste nosso caso, podemos ver que todas as colunas estão devidamente preenchidas (vide que linha *count* possui o mesmo número de linhas).

- Dados Pessoais
"""

df_pessoais.info()

"""Diferentemente do *dataframe* anterior, este **possui** linhas nulas e 5 colunas do tipo objeto. Vamos identificar se estas colunas são do tipo categóricas:"""

colunas_tipo_obj = df_pessoais.select_dtypes(include='object').columns.to_list()

for col in colunas_tipo_obj:
    print(f'Coluna: {col} / Valores Categóricos: {df_pessoais[col].unique()}')

"""Como podemos ver, a coluna `cod_cliente` representa um identificador para cada linha deste *dataset*.

As colunas ID devem ser usadas apenas para identificar e indexar registros, não para a análise preditiva. Para construir modelos preditivos eficazes, deve-se focar em variáveis que têm uma relação lógica e direta com a variável alvo. Por este motivo, ela não será considerada futuramente.

Além disto, duas das colunas categóricas possuem valores `nan` (Not-a-Number).

Colunas com valores NaN (Not a Number) precisam ser tratadas por várias razões em análise de dados e machine learning:

1. **Algoritmos de Machine Learning**:
   - Muitos algoritmos de machine learning não conseguem lidar diretamente com valores NaN e podem falhar ou produzir resultados incorretos se valores NaN estiverem presentes nos dados de entrada.

2. **Cálculos Estatísticos**:
   - Operações estatísticas básicas, como média, mediana e variância, podem ser afetadas por valores NaN, resultando em cálculos imprecisos ou inválidos.

3. **Impacto na Análise**:
   - Valores NaN podem distorcer análises e visualizações de dados. Gráficos, tabelas e outras representações podem ser incorretas ou incompletas se os valores NaN não forem tratados adequadamente.

4. **Consistência dos Dados**:
   - Valores NaN podem indicar a ausência de dados, mas também podem ser causados por erros de entrada ou problemas de coleta de dados. Tratar esses valores ajuda a manter a consistência e a qualidade dos dados.

5. **Preparação para Modelagem**:
   - A preparação adequada dos dados é crucial para a construção de modelos preditivos robustos. Tratar valores NaN é uma parte essencial do processo de pré-processamento de dados.

Para este trabalho, usaremos vamos preencher valores NaN com médias ou modas caso a coluna seja numérica ou categórica, respectivamente.

Agora vamos analisar as colunas numéricas deste *dataframe*.
"""

df_pessoais.describe()

"""Diferentemente do dataframe anterior, os dados pessoais possuem colunas com valores nulos. Isto pode ser visto pelo contador de linhas (count) que mostra que todas as colunas tem quantidades diferentes do número total de linhas deste *dataframe*: `4840`

- Estados Brasileiros

Por fim, o *dataframe* de estados, por ser pequeno é possível de analisa-lo apenas com olhar. Temos todos os estados brasileiros bem descritos, com suas respectivas siglas e regiões, sem presença de dado nulo ou mal digitado.
"""

df_estados

"""## 4.2 Tratamento de dataframes

Conforme visto anteriormente, apenas os dados pessoais apresentam alguns tipos de erros. Vamos analisa-los de perto buscando entende-los e trata-los.

As colunas que contem algum valor nulo são:
- Salário
- Idade
- Estado Civil
- Profissão
- Número de Cartões de Crédito

Estes tipos de informações são extremamente específicas do cliente. Assumir a substituição destes, pela média ou moda, pode acarretar em insights enviesados. Antes de realizar estas substituições, vamos analisar a representatividade destes dados perante o restante.
"""

df_pessoais_semNaN = df_pessoais.dropna()
df_pessoais_semNaN.shape

print(f'O tamanho do dataset, sem valores nulos, representa uma diminuição de {(1 - df_pessoais_semNaN.shape[0] / df_pessoais.shape[0]) * 100:.2f}% do global.')

"""Apesar do percentual está abaixo de 2%, realizaremos substituições considerando as seguintes regras:
1. Moda para as variáveis categóricas.
1. Média para as variáveis numéricas.

Para isto consideraremos a seguinte função que irá preencher locais onde há NaN com moda ou média conforme o input do usuário.
"""

def preenchimento(data, cols, operacao):
    for col in cols:
        if operacao == 'moda':
                data[col].fillna(value=data[col].mode()[0],inplace=True)
        if operacao == 'media':
                data[col].fillna(value=data[col].mean(),inplace=True)
    return data

col_numericas = ['Salário', 'Idade', 'Número de Cartões de Crédito']
col_categoricas = ['Estado Civil', 'Profissão']

df_pessoais = preenchimento(df_pessoais, col_numericas, 'media')
df_pessoais = preenchimento(df_pessoais, col_categoricas, 'moda')
df_pessoais.info()

"""Com os valores nulos devidamente preenchidos, podemos rodar o código abaixo para garantir que não há mais linhas nulas neste *dataframe*."""

# Código para identificar linhas com valores nulos
df_pessoais[pd.isnull(df_pessoais).any(axis=1)].shape[0]

"""Algo incomum foi visto anteriormente, porém não foi realçado. A coluna cod_cliente, por conter apenas dígitos identificadores, deveria possuir um tipo numérico ao invés de objeto.

Pode haver alguma linha diferente de número que esteja "sujando" esta coluna.
"""

df_pessoais[~df_pessoais['cod_cliente'].str.isnumeric()]

"""O código acima buscou por linhas onde a coluna cod_cliente não seja numérica.

Encontramos apenas uma linha com esta característica. Nos próximos tópicos, iremos unir os três datasets em um único e a coluna `cod_cliente` será nossa chave primária. Por conta disto, vamos remover esta linha para que tenhamos todos dígitos identificadores para junções futuras.
"""

df_pessoais = df_pessoais[df_pessoais['cod_cliente'].str.isnumeric()]
df_pessoais.shape

df_pessoais['cod_cliente'] = df_pessoais['cod_cliente'].astype('int64')
df_pessoais.info()

"""Partindo para o próximo passo de análise, vamos averiguar se o *dataframe* possui duplicatas e remove-las, caso haja.

Duplicatas podem distorcer a análise dos dados, levando a resultados incorretos. Por exemplo, duplicatas podem afetar a média, mediana ou outros cálculos estatísticos. Dados duplicados podem indicar problemas de qualidade dos dados, como erros na coleta ou entrada de dados. Removê-los ajuda a garantir que os dados sejam precisos e consistentes.

Por fim, trabalhar com dados duplicados aumenta desnecessariamente o volume de dados, o que pode tornar os processos de computação mais lentos e consumir mais recursos.
"""

duplicados = df_pessoais.duplicated()
df = df_pessoais[duplicados].shape

df_pessoais_semDuplic = df_pessoais.drop_duplicates()
df_pessoais_semDuplic.info()

"""Como é possível visualizar, houve uma redução de mais 800 linhas. Vamos aproveitar e remover linhas duplicadas do *dataset* de Características Físicas também."""

duplicados = df_carac_fisica.duplicated()
df_carac_fisica[duplicados].shape

df_carac_fisica_semDuplic = df_carac_fisica.drop_duplicates()
df_carac_fisica_semDuplic.info()

"""## 4.3 União de *dataframes*

Agora que todos os *datasets* estão devidamente tratados, vamos uni-los em um único conjunto coeso para iniciar as análises e elaboração de insights.
O *dataframe* de dados pessoais será a tabela base, pois já possui chaves (*keys*) que se conectam com os demais.

### 4.3.1 Junção Dados Pessoais X Característica Física
"""

dados = df_pessoais_semDuplic.merge(df_carac_fisica_semDuplic, left_on = 'cod_cliente', right_on='ID Cliente', how='left')
dados.head()

"""### 4.3.2 Tabela anterior X Estados"""

dados = dados.merge(df_estados, left_on='cod_estado', right_on='id_estado', how='left')
dados.head()

"""### 4.3.3 Filtrando colunas necessárias

Vamos analisar todas as colunas deste novo *dataframe* criado e selecionar aquelas que serão úteis em nossa análise.
"""

colunas = dados.columns.to_list()
colunas

colunas_filtradas = [   'Escolaridade',
                        'Tem Filhos',
                        'Salário',
                        'Idade',
                        'Estado Civil',
                        'Profissão',
                        'Número de Cartões de Crédito',
                        'Cor do Cabelo',
                        'Cor dos Olhos',
                        'Cor da Pele',
                        'Altura (cm)',
                        'Peso (kg)',
                        'Tatuagens',
                        'Piercings',
                        'Tipo Sanguíneo',
                        'Tipo de Pele',
                        'estado',
                        'regiao']

"""Separação em colunas numéricas e categóricas"""

colunas_categoricas = []
colunas_numericas = []
for col in colunas_filtradas:
    if is_string_dtype(dados[col]):
        colunas_categoricas.append(col)
    else:
        colunas_numericas.append(col)

colunas_filtradas = colunas_categoricas + colunas_numericas
dados_limpos = dados[colunas_filtradas]
dados_limpos.head()

"""# 5. Análise do conjunto de dados tratados

Agora que temos um *dataframe* limpo e com os dados mais interessantes, vamos realizar nossas análises e buscar retirar percepções.

Como foi dito anteriormente, o conjunto de dados coletados representam informações fictícias sobre os clientes de uma empresa, abrangendo uma ampla gama de aspectos, desde características demográficas até dados físicos e de saúde.

Com isto, podemos retirar algumas informações iniciais a respeito de nosso público:

## 5.1. Insights iniciais

Vamos inicialmente analisar o *dataframe* respondendo algumas perguntas simples de modo a nos acostumarmos com as informações. A partir daí iremos nos aprofundar nas análises.

1. Qual é a cor de cabelo mais comum entre os clientes?
"""

cor_do_cabelo = dados_limpos['Cor do Cabelo'].value_counts()
cor_do_cabelo

"""Considerando que pessoas de cabelos ruivos representam cerca de 2% da população mundial (vide [link](https://www.worldatlas.com/articles/what-percentage-of-the-world-population-have-red-hair.html)), é provável que esta coleta foi executada em regiões norte-oeste da Europa, onde este contingente é maior.

2.  Existe alguma correlação entre a altura e o peso dos clientes?
"""

sns.heatmap(dados_limpos[['Altura (cm)', 'Peso (kg)']].corr(), annot=True)

"""No geral, o peso de uma certa população tem certo vínculo com sua respectiva altura dado que a maioria tende a ter um padrão de corpo saudável, sendo nem extremamente magro ou extremamente obeso. Analisaremos futuramente este ponto com mais afinco.

3.  Qual é a distribuição de tatuagens entre os clientes?
"""

tatoo = dados_limpos['Tatuagens'].value_counts()
tatoo

"""É curioso de se imaginar que a quantidade de tatuagens corporais variem de maneira não convencional. Vemos que a maioria da população possui 3 tatuagens, sendo quase que equiparado com o pessoal que possui duas e logo após, a população sem tatuagens, para no fim as pessoas com apenas uma tatuagem serem minoria.

É possível inferir que uma vez que a pessoa realiza uma tatuagem, a mesma se empolga e deseja fazer mais com o passar do tempo.

4.  Quantos clientes têm o tipo sanguíneo AB-
"""

tipo_sanguineo = dados_limpos['Tipo Sanguíneo'].value_counts()
tipo_sanguineo

tipo_sanguineo_perc = dados_limpos['Tipo Sanguíneo'].value_counts()/dados_limpos['Tipo Sanguíneo'].value_counts().sum()
tipo_sanguineo_perc

"""O tipo sanguíneo mais comum na população mundial é o O+, chegando a 42% da população ([World Atlas](https://www.worldatlas.com/articles/what-are-the-different-blood-types.html)). Em termos percentuais, esta população de equipara bastante, o comum seria ver um percentual maior destinado para os O+. Este tipo de percentual acaba de mostrando comum nas análises feitas.

5.  Qual é o tipo de pele mais prevalente entre os clientes?
"""

tipo_pele = dados_limpos['Tipo de Pele'].value_counts()
tipo_pele

"""Tipos de pele pode ter grande validade para empresas cosméticas que buscam vender seu produto cada vez mais para o publico alvo. Ver que a maioria da população possui pele do tipo Normal ou mista será de grande importância.

6.  Qual é o nível de escolaridade predominante entre os clientes?
"""

escolaridade = dados_limpos['Escolaridade'].value_counts()
escolaridade

"""Esta escala acaba sendo similar a alguns países emergentes onde já foi vencida a barreira do ensino fundamental provocando uma população que consegue finalizar o ensino médio. No entanto, esta realidade não se difere um pouco com o brasil onde cerca de metade da população já conseguiu ter formação básica completa (vide [link](https://www.poder360.com.br/educacao/545-dos-brasileiros-tem-formacao-basica-completa-diz-pnad/)). Vale salientar que estes tipos de analises são do tipo quantitativos e não qualitativos.

10. Quais são as profissões mais comuns entre os clientes?
"""

profissao = dados_limpos['Profissão'].value_counts()
profissao

"""Por haver uma maioria da população de estudantes, é provável que esta coleta tenha sido feita próximo a setores universitários ou de estágios profissionais, visto que a quantidade de estudantes se equipara com as quantidades dos profissionais que vem logo após.

## 5.2. Análise gráfica do dataset

Com as inferências iniciais, partiremos para análise gráfica do conjunto de dados tratados.

1. Contagem total de tipos sanguíneos.
"""

# Criando o gráfico de barras
plt.figure(figsize=(8, 5))
dados_limpos['Tipo Sanguíneo'].value_counts().plot(kind='bar')

# Adicionando títulos e rótulos
plt.title('Contagem Total de Tipos Sanguíneos')
plt.xlabel('Tipos Sanguíneos')
plt.ylabel('Contagem')

# Exibindo o gráfico
plt.show()

"""Como foi dito no item 4 das seção anterior, é curioso termos uma população onde o tipo sanguíneo O+ não seja maioria. Além disto, as quantidades estão bastante perto umas das outras.

2. Contagem de tipos de pele.
"""

# Criando o gráfico de barras
plt.figure(figsize=(8, 5))
dados_limpos['Tipo de Pele'].value_counts().plot(kind='bar')

# Adicionando títulos e rótulos
plt.title('Contagem Total de Tipo de Pele')
plt.xlabel('Tipo de Pele')
plt.ylabel('Contagem')

# Exibindo o gráfico
plt.show()

"""Segue o mesmo tipo de inferência dita anteriormente e das já pontuadas no item 5 da seção anterior.

3. Contagem de cores de pele.
"""

# Criando o gráfico de barras
plt.figure(figsize=(8, 5))
dados_limpos['Cor da Pele'].value_counts().plot(kind='bar')

# Adicionando títulos e rótulos
plt.title('Contagem Total de Cor da Pele')
plt.xlabel('Cor da Pele')
plt.ylabel('Contagem')

# Exibindo o gráfico
plt.show()

"""Apesar de ser comum dizer que o Brasil é um país bastante miscigenado, na realidade este percentual possui suas maiorias. Segundo o [IBGE](https://educa.ibge.gov.br/jovens/conheca-o-brasil/populacao/18319-cor-ou-raca.html#:~:text=A%20pesquisa%20revelou%20ainda%20que,1%20mil%29%20se%20declararam%20amarelas.), cerca de 89% da população se declara branca ou parda, seguindo de negra e uma parcela mínima indígena. O *dataset* mostrado pode conter uma coleta muito específica da população ou dados equivocados.

4. Frequência de escolaridade por região.
"""

freq_df = dados_limpos.groupby(['regiao', 'Escolaridade']).size().reset_index(name='Contagem')

# Configurando o estilo do seaborn
sns.set(style="whitegrid")

# Criando um gráfico de barras
plt.figure(figsize=(12, 8))
grafico = sns.barplot(x='regiao', y='Contagem', hue='Escolaridade', data=freq_df, palette='viridis')

# Adicionando título e rótulos
grafico.set_title('Frequência de Escolaridade por região')
grafico.set_xlabel('Região')
grafico.set_ylabel('Contagem')

# Exibindo o gráfico
plt.show()

"""Ao separar a escolaridade por região, fica claro a diferencia populacional entre regiões. No entanto, segundo o [IBGE](https://agenciadenoticias.ibge.gov.br/agencia-noticias/2012-agencia-de-noticias/noticias/37237-de-2010-a-2022-populacao-brasileira-cresce-6-5-e-chega-a-203-1-milhoes#:~:text=O%20Sudeste%20continua%20sendo%20a,9%25%20dos%20habitantes%20do%20pa%C3%ADs.), o Sudeste supera o Nordeste em população o que difere do mostrado no gráfico acima.

Além disso, o índice de escolaridade se encontra bastante equiparado nas regiões o que segundo o [Poder360](https://www.poder360.com.br/educacao/545-dos-brasileiros-tem-formacao-basica-completa-diz-pnad/), este percentual pode variar quantitativamente.

5. Frequência de filhos por região.
"""

freq_df = dados_limpos.groupby(['regiao', 'Tem Filhos']).size().reset_index(name='Contagem')

# Configurando o estilo do seaborn
sns.set(style="whitegrid")

# Criando um gráfico de barras
plt.figure(figsize=(12, 8))
grafico = sns.barplot(x='regiao', y='Contagem', hue='Tem Filhos', data=freq_df, palette='viridis')

# Adicionando título e rótulos
grafico.set_title('Frequência de clientes que tem filhos por região')
grafico.set_xlabel('Região')
grafico.set_ylabel('Contagem')

# Exibindo o gráfico
plt.show()

"""Os quantitativos de pessoas que têm filhos e que não têm está bastante equiparado o que pode diferir da realidade.

6. Média salarial por região.
"""

# Media salarial por região
freq_df = dados_limpos.groupby('regiao')['Salário'].mean().reset_index(name='quantidade')

# Configurando o estilo do seaborn
sns.set(style="whitegrid")

# Criando um gráfico de barras
plt.figure(figsize=(12, 8))
grafico = sns.barplot(x='regiao', y='quantidade', data=freq_df, palette='viridis')

# Adicionando título e rótulos
grafico.set_title('Média salarial por região')
grafico.set_xlabel('Região')
grafico.set_ylabel('Salário em R$')

# Usando StrMethodFormatter para adicionar o cifrão aos rótulos do eixo y
grafico.yaxis.set_major_formatter('R${x:1.2f}')

# Personalizando a aparência dos rótulos
grafico.yaxis.set_tick_params(which='major', labelcolor='green', labelleft=False, labelright=True)

# Exibindo o gráfico
plt.show()

"""Dadas informações quanto ao custo de vida, local de moradia e densidade populacional, os salários nas regiões podem se diferenciar especialmente se a profissão for muito qualificada. Para profissões mais comuns a média inicial gira em torno de R\$ 1.700,00 a R\$ 2.000,00 conforme link do [g1](https://g1.globo.com/trabalho-e-carreira/noticia/2024/02/07/as-ocupacoes-com-os-100-maiores-e-os-100-menores-salarios-de-contratacao-no-pais-em-2023.ghtml).

7. Variação do IMC de acordo com o peso.

Antes de plotar este gráfico, vamos criar uma nova coluna calculando o Índice de massa corpórea linha a linha.
"""

# Calcular IMC
dados_limpos['IMC'] = dados_limpos['Peso (kg)'] / (dados_limpos['Altura (cm)']/100)**2
dados_limpos.head()
colunas_numericas.append('IMC')

"""![Imagem](https://superafarma.com.br/wp-content/uploads/2022/12/Supera-Farma-Tabela-IMC-Classificacao.png)"""

sns.scatterplot(data=dados_limpos, x='Peso (kg)', y='IMC')
plt.title('IMC x Peso')
plt.show()

"""Como o IMC é linearmente dependente do peso, supõe-se que o gráfico acabe seguindo uma tendencia, neste caso é de subida. No entanto, o índice sofre uma variação muito grande mostrando pessoas altamente subnutridas e extremamente obesas."""

sns.scatterplot(data=dados_limpos, x='Altura (cm)', y='IMC')
plt.title('IMC x Altura')
plt.show()

"""O gráfico da altura se comporta de maneira inversamente proporcional e continua a mostrar uma variação de dados extrema.

8. Média salarial por Idade.
"""

# Media salarial por idade
freq_df = dados_limpos.groupby('Idade')['Salário'].mean().reset_index(name='quantidade')

# Configurando o estilo do seaborn
sns.set(style="whitegrid")

# Criando um gráfico de barras
plt.figure(figsize=(12, 8))
grafico = sns.scatterplot(x='Idade', y='Salário', data=dados_limpos, palette='viridis')

# Adicionando título e rótulos
grafico.set_title('Salário por idade')
grafico.set_xlabel('Idade')
grafico.set_ylabel('Salário em R$')

# Usando StrMethodFormatter para adicionar o cifrão aos rótulos do eixo y
grafico.yaxis.set_major_formatter('R${x:1.2f}')

# Personalizando a aparência dos rótulos
grafico.yaxis.set_tick_params(which='major', labelcolor='green', labelleft=False, labelright=True)

# Exibindo o gráfico
plt.show()

"""O gráfico mostra que não há relação calculável entre idade e salário, qualquer idade pode receber o mesmo range de salários, o que sabemos na realidade há uma certa tendência de que quanto maior a idade, maior o salário, dado que o profissional ganho experiência com o passar dos anos.

9. Média de altura e peso por estado.

Antes de plotar o gráfico, vamos realizar um filtro do tipo groupby no *dataframe* para pegar as informações necessárias.
"""

# Agrupar por estado e calcular a média de altura e o respectivo peso
df_altura_peso_por_estado = dados_limpos.groupby('estado')[['Altura (cm)', 'Peso (kg)']].mean().reset_index()
df_altura_peso_por_estado.head()

# Gráfico: Calcular a altura e peso médio por estado
fig, ax = plt.subplots(2, 1, figsize=(14, 10))

paleta = sns.color_palette('viridis', len(df_altura_peso_por_estado['estado'].unique()))
paleta_clr = sns.color_palette('colorblind', len(df_altura_peso_por_estado['estado'].unique()))

sns.barplot(x='estado', y='Altura (cm)', data=df_altura_peso_por_estado, ax=ax[0],palette=paleta)
ax[0].set_title('Altura Média por Estado')
ax[0].set_ylabel('Altura Média (cm)')
ax[0].set_xlabel('Estado')
ax[0].tick_params(axis='x', rotation=90)

sns.barplot(x='estado', y='Peso (kg)', data=df_altura_peso_por_estado, ax=ax[1],palette=paleta_clr)
ax[1].set_title('Peso Médio por Estado')
ax[1].set_ylabel('Peso Médio (kg)')
ax[1].set_xlabel('Estado')
ax[1].tick_params(axis='x', rotation=90)

plt.tight_layout()
plt.show()

"""Vemos que a altura e peso se diferem pouco entre os estados do Brasil. Entretanto, o esperado seria que esta variação fosse maior dado que o Brasil é um país continental e foi colonizado por diversas etnias ao longo do tempo. Por exemplo, a região norte possui uma quantidade maior de nativos brasileiros enquanto que a região sul/sudeste foi bastante colonizada por culturas europeias e asiáticas. Apenas este ponto já seria o suficiente para visualizar diferenças na altura da média da população.

10. Média salarial de acordo com a escolaridade.

Antes de plotar o gráfico, vamos realizar um filtro do tipo groupby no *dataframe* para pegar as informações necessárias.
"""

# Qual é a relação entre escolaridade e salário médio dos clientes?
df_salario_por_escolaridade = dados_limpos.groupby('Escolaridade')['Salário'].mean().round(2).reset_index()
df_salario_por_escolaridade = df_salario_por_escolaridade.sort_values(by='Salário')
display(df_salario_por_escolaridade)

# Gráfico: Calcular média salarial por escolaridade em orientação horizontal
plt.figure(figsize=(10, 6))
paleta = sns.color_palette('pastel', len(df_altura_peso_por_estado['estado'].unique()))
sns.barplot(x='Salário', y='Escolaridade', data=df_salario_por_escolaridade, orient='h', palette=paleta)
plt.title('Média Salarial por Escolaridade')
plt.ylabel('Escolaridade')
plt.xlabel('Salário Médio (R$)')
plt.show()

"""Curiosamente, a média salarial não segue o senso comum de que o aumento salarial segue o aumento hierárquico da escolaridade do indivíduo. Além disso, os salários estão muito próximos independente do nível de escolaridade.

11. Quantidade de cor dos olhos de acordo com região.

Antes de plotar o gráfico, vamos realizar um filtro do tipo groupby no *dataframe* para pegar as informações necessárias.
"""

# Distribuição da cor dos olhos entre diferentes regiões?
df_cor_olhos_por_regiao = dados_limpos.groupby(['regiao', 'Cor dos Olhos']).size().reset_index(name='Quantidade')
df_cor_olhos_por_regiao.sample(5)

# Gráfico: Barras empilhadas para cor dos olhos por região
plt.figure(figsize=(12, 8))
paleta = sns.color_palette('Set2')
sns.barplot(x='regiao', y='Quantidade', hue='Cor dos Olhos', data=df_cor_olhos_por_regiao, palette = paleta)
plt.title('Distribuição da Cor dos Olhos por Região')
plt.xlabel('Região')
plt.ylabel('Contagem')
plt.legend(title='Cor dos Olhos')
plt.show()

"""As cores dos olhos possuem uma boa variação nas populações regionais, especialmente no Centro-Oeste. A quantidade de pessoas segue mesma curva dos gráficos anteriores onde mostra uma densidade populacional maior no Nordeste.

12. Agrupamento de média de tatuagens e piercings por faixa etárias.

Antes de plotar o gráfico, vamos realizar um filtro do tipo groupby no *dataframe* para pegar as informações necessárias.
"""

# Verificar se existe correlação entre a quantidade de tatuagens e piercings com a idade dos clientes?
### Agrupar por faixa etária e calcular a média de tatuagens e piercings
df_tattoos_piercings_por_idade = dados_limpos
df_tattoos_piercings_por_idade['Idade'] = df_tattoos_piercings_por_idade['Idade'].round().astype(int)
df_tattoos_piercings_por_idade = df_tattoos_piercings_por_idade.groupby('Idade')[['Tatuagens', 'Piercings']].mean().reset_index()
df_tattoos_piercings_por_idade['Tatuagens'].fillna(0, inplace=True)
df_tattoos_piercings_por_idade['Piercings'].fillna(0, inplace=True)
df_tattoos_piercings_por_idade.head()

# Gráfico: Mostrar a média de tatuagens e piercings por faixa etária
plt.figure(figsize=(30, 6))
sns.barplot(x='Idade', y='Tatuagens', data=df_tattoos_piercings_por_idade, color='powderblue', label='Tatuagens')
sns.barplot(x='Idade', y='Piercings', data=df_tattoos_piercings_por_idade, color='steelblue', label='Piercings')
plt.title('Média de Tatuagens e Piercings por Faixa Etária')
plt.xlabel('Faixa Etária')
plt.ylabel('Média')
plt.legend()
plt.show()

"""A  média de tatuagens e piercings segue dentro de uma faixa mínima e máxima, não importando a idade do indivíduo.

13. Variação do tipo sanguíneo por região.

Antes de plotar o gráfico, vamos realizar um filtro do tipo groupby no *dataframe* para pegar as informações necessárias.
"""

#Qual é a distribuição do tipo sanguíneo por região? Agrupando por região e tipo sanguíneo.

df_tipo_sanguineo_por_regiao = dados_limpos.groupby(['regiao', 'Tipo Sanguíneo']).size().reset_index(name='Quantidade')
df_tipo_sanguineo_por_regiao.sample(5)

# Gráfico: Mostrar a quantidade de tipo sanguíneo por região
cores = {'A+': 'blue', 'A-': 'red', 'B+': 'green', 'B-': 'orange', 'AB+': 'purple', 'AB-': 'brown', 'O+': 'pink', 'O-': 'gray'}

plt.figure(figsize=(10, 6))

# Rodando um Loop pelos tipos sanguíneos
for tipo_sanguineo, cor in cores.items():
    # Filtrando os dados para o tipo sanguíneo atual
    dados_tipo_sanguineo = df_tipo_sanguineo_por_regiao[df_tipo_sanguineo_por_regiao['Tipo Sanguíneo'] == tipo_sanguineo]
    # Plotando a linha para o tipo sanguíneo atual
    plt.plot(dados_tipo_sanguineo['regiao'], dados_tipo_sanguineo['Quantidade'], marker='o', label=tipo_sanguineo, color=cor)

plt.title('Tipo Sanguíneo por Região')
plt.xlabel('Região')
plt.ylabel('Quantidade')
plt.gca().spines[['top', 'right']].set_visible(False)
plt.xticks(rotation=45)
plt.legend(title='Tipo Sanguíneo')
plt.grid(True)
plt.show()

"""Como mostrado anteriormente, o tipo O- é mostrado como quantidade máxima no nordeste e sudeste, onde o esperado seria visualizarmos O+ como máximo.

14. Distribuição de cor do cabelo dos clientes por região.

Antes de plotar o gráfico, vamos realizar um filtro do tipo groupby no *dataframe* para pegar as informações necessárias.
"""

df_cor_cabelo_por_regiao = dados_limpos.groupby(['regiao', 'Cor do Cabelo']).size().reset_index(name='Quantidade')
df_cor_cabelo_por_regiao.sample(5)

# Gráfico: Mostrar a quantidade de tipos de cabelos por região

cores_cabelo = {'Castanho': 'brown', 'Loiro': 'yellow', 'Preto': 'black', 'Ruivo': 'red'}

plt.figure(figsize=(10, 6))

grafico = sns.barplot(x='regiao', y='Quantidade', hue='Cor do Cabelo', data=df_cor_cabelo_por_regiao, palette=cores_cabelo)

plt.title('Tipo Cabelos por Região')
plt.xlabel('Região')
plt.ylabel('Quantidade')
plt.gca().spines[['top', 'right']].set_visible(False)
plt.xticks(rotation=45)
plt.legend(title='Tipos de Cabelos')
plt.grid(True)
plt.show()

"""As cores de cabelo possuem baixa variação de quantidade, além da população de ruivos no Brasil ser muito alta para a quantidade real.

15. Relação entre escolaridade e quantidade de filhos.

Antes de plotar o gráfico, vamos realizar um filtro do tipo groupby no *dataframe* para pegar as informações necessárias.
"""

# Agrupar por escolaridade e calcular a média de quantidade de filhos

df_filhos_por_escolaridade = dados_limpos.groupby('Escolaridade')['Tem Filhos'].count().reset_index()

df_filhos_por_escolaridade = df_filhos_por_escolaridade.sort_values(by='Tem Filhos', ascending=False)
display(df_filhos_por_escolaridade)

# Gráfico: Quantidade Total de filhos por escolaridade (ordem decrescente)
plt.figure(figsize=(10, 6))
paleta_escolaridade = sns.color_palette('Set2')
sns.barplot(x='Escolaridade', y='Tem Filhos', data=df_filhos_por_escolaridade,palette = paleta_escolaridade)
plt.title('Média de Quantidade de Filhos por Escolaridade')
plt.ylabel('Média de Quantidade de Filhos')
plt.xlabel('Escolaridade')
plt.xticks(rotation=45)
plt.show()

"""A quantidade de filhos tem baixa variação entre as escolaridades. Entretanto, a ordem descendente em que aparece, faz sentido onde hierarquias escolares maiores possuem menos filhos.

16. Relação entre salário e cor da pele dos clientes.

Antes de plotar o gráfico, vamos realizar um filtro do tipo groupby no *dataframe* para pegar as informações necessárias.
"""

# Agrupar por tipo de pele e calcular a média de salário
df_salario_por_cor_pele = dados_limpos.groupby('Cor da Pele')['Salário'].mean().round(2).reset_index()
df_salario_por_cor_pele = df_salario_por_cor_pele.sort_values(by='Salário')
print(df_salario_por_cor_pele)

# Gráfico: Calcular média salarial por escolaridade
plt.figure(figsize=(10, 6))
paleta_cor_pele = sns.color_palette('Spectral')
sns.barplot(x='Salário', y='Cor da Pele', data=df_salario_por_cor_pele, orient='h', palette=paleta_cor_pele)
plt.title('Média de Salário por Cor de Pele')
plt.ylabel('Cor da Pele')
plt.xlabel('Salário Médio (R$)')
plt.show()

"""Diferentemente do esperado, indivíduos com cor de pele amarela possuem maior media salarial, seguido por pessoas brancas, negras e pardas, finalizando com indígenas.

17. Proporção de clientes com cartões de crédito por estado civil.

Antes de plotar o gráfico, vamos realizar um filtro do tipo groupby no *dataframe* para pegar as informações necessárias.
"""

# Calcular a proporção de clientes com cartões de crédito por estado civil

df_prop_cartao_por_estado_civil = dados_limpos.groupby('Estado Civil')['Número de Cartões de Crédito'].apply(lambda x: (x > 0).mean()).reset_index(name='proporcao_cartao')
df_prop_cartao_por_estado_civil

# Gráfico: Calcular a proporção de clientes com cartões de crédito por estado civil (Pizza)
plt.figure(figsize=(8, 8))
plt.pie(df_prop_cartao_por_estado_civil['proporcao_cartao'], labels=df_prop_cartao_por_estado_civil['Estado Civil'], autopct='%1.1f%%', startangle=140)
plt.title('Proporção de Clientes com Cartões de Crédito por Estado Civil')
plt.axis('equal')
plt.show()

"""A proporção se encontra bastante igualitária da quantidade de cartão por estado civil.

18. Quantidade de pessoas por escolaridade e seus respectivo Estado Civil.

Antes de plotar o gráfico, vamos realizar um filtro do tipo groupby no *dataframe* para pegar as informações necessárias.
"""

df_escolaridade_estado_civil = dados_limpos.groupby(['Escolaridade', 'Estado Civil']).size().reset_index(name='Quantidade_EES')

df_escolaridade_estado_civil.sample(5)

# Gráfico: Mostrar a relação da quantidade de pessoas pela sua escolaridade e estados civil

cores_estado_civil = {'Casado': 'blue', 'Divorciado': 'red', 'Solteiro': 'green', 'Viúvo': 'orange'}

plt.figure(figsize=(10, 6))

# Rodando um Loop pelos tipo de estado civil
for tipo_estado_civil, cor in cores_estado_civil.items():
    # Filtrando os dados para o tipo de estado civil atual
    dados_tipo_estado_civil = df_escolaridade_estado_civil[df_escolaridade_estado_civil['Estado Civil'] == tipo_estado_civil]
    # Plotando a linha para o tipo estado civil
    plt.plot(dados_tipo_estado_civil['Escolaridade'], dados_tipo_estado_civil['Quantidade_EES'], marker='o', label=tipo_estado_civil, color=cor)

plt.title('Relação entre Escolaridade por Estado Civil')
plt.xlabel('Escolaridade')
plt.ylabel('Quantidade Escolaridade')
plt.gca().spines[['top', 'right']].set_visible(False)
plt.xticks(rotation=45)
plt.legend(title='Estado Civil')
plt.grid(True)
plt.show()

"""A quantidade de escolaridade é basicamente liderada por pessoas solteiras. Isto é esperado dado que a maioria das pessoas iniciam a vida acadêmica antes de engajar em relacionamentos sérios.

## 5.3. Gráfico de correlação das colunas numéricas

Para finalizar nossa análise do *dataset* vamos plotar a tabela de correlação para buscar visualizar as correlações que há entre as colunas numéricas. As colunas categorias podem entrar futuramente nesta tabela após passagem para número com algoritmos tipo One-Hot conforme explicado no inicio deste notebook.
"""

plt.figure(figsize=(10, 6))
sns.heatmap(dados_limpos[colunas_numericas].corr(), annot=True)
plt.show()

"""Conforme mostrado no mapa de calor, praticamente todas as colunas possuem relação extremamente baixas. A exceção a esta regra é a coluna IMC que possui relação com Peso e Altura dado que estas variáveis fazem parte da equação.

Este mapa de calor mostra também a falta de sentido entre as análises comparativas que foram relatadas na seção anterior.

# 6. Conclusões

Com base na análise detalhada dos dados, é possível concluir os seguintes pontos:

1. **Distribuição Populacional Incongruente**: A análise dos dados revela que certas características populacionais, como a distribuição de cores de pele e tipos de cabelo, não correspondem às expectativas regionais. Por exemplo, há uma quantidade desproporcionalmente alta de pessoas ruivas em regiões onde isso é raro.

2. **Baixa Variação em Médias Salariais**: As médias salariais apresentadas no dataset mostram pouca variação, independentemente de faixa etária, nível de escolaridade ou região de moradia. Esta baixa variação é estatisticamente improvável e sugere problemas na coleta ou geração dos dados.

3. **Correlação Entre Variáveis**: O mapa de correlação confirma que há baixíssima correlação entre as variáveis analisadas, exceto pela relação esperada entre IMC, peso e altura. A falta de correlação significativa entre as outras variáveis reforça a ideia de que os dados podem não ser confiáveis.

4. **Qualidade dos Dados**: As inconsistências e incongruências observadas indicam que os dados podem ter sido coletados de forma inadequada ou que podem ser fictícios. Isso compromete a validade das análises e qualquer inferência baseada nesses dados.

Dito isto, temos as seguintes recomendações:

- **Revisão do Processo de Coleta de Dados**: Recomenda-se uma revisão completa do processo de coleta de dados para garantir que os dados sejam representativos e precisos.
- **Verificação da Integridade dos Dados**: É crucial verificar a integridade dos dados e, se necessário, realizar uma limpeza para remover ou corrigir entradas errôneas.
- **Reanálise com Dados Confiáveis**: Uma vez garantida a qualidade dos dados, novas análises devem ser conduzidas para obter insights precisos e acionáveis.

Essas ações são essenciais para assegurar que as análises futuras sejam baseadas em dados sólidos e representativos, proporcionando uma base confiável para a tomada de decisões estratégicas.
"""